{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0bd2ca9-a64e-44a4-b065-ba27c7564b1a",
   "metadata": {
    "id": "c0bd2ca9-a64e-44a4-b065-ba27c7564b1a"
   },
   "source": [
    "# 6 - Ranking and Recommender Models\n",
    "## CSCI E-108\n",
    "### Steve Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dceee9-fab6-41a0-ba03-8e301333bb3c",
   "metadata": {
    "id": "a3dceee9-fab6-41a0-ba03-8e301333bb3c"
   },
   "source": [
    "Recommender algorithms are a vast and complex subject. A comprehensive treatment is impractical in a reasonable time frame. Rather, the exercises in this notebook are formulated to increase and evaluate your understanding of the basic principles of recommender algorithms. \n",
    "\n",
    "> **Note:** The code provided in this notebook was created and tested in Google Colab. With a few file-system specific modification this notebook should execute in your local environment if you so choose.  \n",
    "\n",
    "We do not use any particular recommender package for these exercises. To get started, execute the code in the cell below to import the required packages.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4b226-b124-4e7b-98dc-98b0fb1c0d1b",
   "metadata": {
    "id": "d5d4b226-b124-4e7b-98dc-98b0fb1c0d1b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from scipy.sparse.linalg import svds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe62ba4-f667-4ae6-9b27-912008125877",
   "metadata": {
    "id": "fbe62ba4-f667-4ae6-9b27-912008125877"
   },
   "source": [
    "> **Attribution:** Much of the code in this notebook has been modified from machine generated code from Microsoft Copilot and Gemini.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110e716-1fb3-496f-bce0-e1666c62c949",
   "metadata": {
    "id": "1110e716-1fb3-496f-bce0-e1666c62c949"
   },
   "source": [
    "## MovieLens Dataset    \n",
    "\n",
    "For the remainder of this notebook you will work the [MovieLens 100k dataset](https://grouplens.org/datasets/movielens/100k/). The dataset contains 100,000 ratings of movies by a number of users.    \n",
    "\n",
    "Execute the code in the cell below to download and unzip the dataset.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c49a9-fc31-4beb-a996-f6ea4c8b7014",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e6c49a9-fc31-4beb-a996-f6ea4c8b7014",
    "outputId": "ff7e7e75-30fa-4813-e6c7-0db4f03d5b98"
   },
   "outputs": [],
   "source": [
    "# URL and target paths\n",
    "url = \"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "zip_path = \"ml-100k.zip\"\n",
    "extract_dir = \"ml-100k\"\n",
    "\n",
    "# Download the dataset\n",
    "print(\"Downloading MovieLens 100k dataset...\")\n",
    "response = requests.get(url)\n",
    "with open(zip_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the zip file\n",
    "print(\"Extracting dataset...\")\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# Clean up zip file\n",
    "os.remove(zip_path)\n",
    "print(f\"\\nDataset extracted to directior: {extract_dir}  With files:\")\n",
    "!ls ml-100k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ee228-dc40-4ec3-8c23-c2a68c553123",
   "metadata": {
    "id": "7e7ee228-dc40-4ec3-8c23-c2a68c553123"
   },
   "source": [
    "Next, execute the code in the cell below to map the zip files to a Pandas dataframe.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79effdb-114e-4158-8f02-1ba5a4939682",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "c79effdb-114e-4158-8f02-1ba5a4939682",
    "outputId": "d4700ddb-ad0a-4d2e-b9f0-7d905b32b152"
   },
   "outputs": [],
   "source": [
    "path = extract_dir + 'u1.base'\n",
    "ratings = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "ratings['datetime'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "print(f\"Number of ratings: {len(ratings)}\")\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1f9fe-b237-4334-ad6f-2c22348698f2",
   "metadata": {
    "id": "ddb1f9fe-b237-4334-ad6f-2c22348698f2"
   },
   "source": [
    "There are three key columns in this data frame, the user ID, the item ID, and the rating. There are a total 100,000 ratings in the data frame.     \n",
    "Next, execute the code in the cell below to instantiate a data frame containing the movie details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757529ed-96fc-40a9-96e5-75e0ee93457b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "757529ed-96fc-40a9-96e5-75e0ee93457b",
    "outputId": "91564b02-09b2-4398-a772-971a2132a5e7"
   },
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'movie id',\n",
    "    'movie title',\n",
    "    'release date',\n",
    "    'video release date',\n",
    "    'IMDb URL',\n",
    "    'unknown',\n",
    "    'Action',\n",
    "    'Adventure',\n",
    "    'Animation',\n",
    "    'Children\\'s',\n",
    "    'Comedy',\n",
    "    'Crime',\n",
    "    'Documentary',\n",
    "    'Drama',\n",
    "    'Fantasy',\n",
    "    'Film-Noir',\n",
    "    'Horror',\n",
    "    'Musical',\n",
    "    'Mystery',\n",
    "    'Romance',\n",
    "    'Sci-Fi',\n",
    "    'Thriller',\n",
    "    'War',\n",
    "    'Western'\n",
    "]\n",
    "\n",
    "movies = pd.read_csv('ml-100k/u.item',  sep='|', encoding='latin-1', header=None)\n",
    "movies.columns = cols\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e984d6-7ef4-48e0-a945-ee98c2bdd170",
   "metadata": {
    "id": "c2e984d6-7ef4-48e0-a945-ee98c2bdd170"
   },
   "source": [
    "## Explore the Dataset    \n",
    "\n",
    "We will now explore the ratings data. To start, execute the code in the cell below to find the number of unique values of the variables in the ratings data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462bfb8-07a8-4870-a135-bccce71ca2da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1462bfb8-07a8-4870-a135-bccce71ca2da",
    "outputId": "42cb39e6-782f-4cce-8030-a27f699df031"
   },
   "outputs": [],
   "source": [
    "unique_vals = ratings.nunique()\n",
    "print(f\"Number of unique values in dataset\\n {unique_vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224ba370-2ff9-48f8-8b67-315d2830d88f",
   "metadata": {
    "id": "224ba370-2ff9-48f8-8b67-315d2830d88f"
   },
   "source": [
    "There are only a limited number of users and movies in the this dataset.    \n",
    "\n",
    "To explore the changes in the number of ratings with time, execute the code in the cell below.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c8a9e-7add-4687-b3d5-02f511a5efa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "ec4c8a9e-7add-4687-b3d5-02f511a5efa6",
    "outputId": "f03fc727-d7e9-45a7-f45a-34dac9c93b23"
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(data=ratings, x='timestamp', y='rating', kind='hex', height=6, ratio=4);\n",
    "sns.regplot(data=ratings,\n",
    "            x='timestamp',\n",
    "            y='rating',\n",
    "            ax=g.ax_joint,\n",
    "            scatter=False,\n",
    "            color='red',\n",
    "            line_kws={'linestyle': '--', \"linewidth\": 1},\n",
    "            ci=95);\n",
    "plt.suptitle(\"Ratings vs. time\\n Regression line in red\", y=1.02);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f7d69-a9a7-4d31-86c0-afd9ca3b2fb2",
   "metadata": {
    "id": "cf7f7d69-a9a7-4d31-86c0-afd9ca3b2fb2"
   },
   "source": [
    "To get a feeling for the distribution of ratings done by the users and the number of ratings for the items run the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7d6ce-766e-4839-ab23-b2d6025fdd7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "3fe7d6ce-766e-4839-ab23-b2d6025fdd7b",
    "outputId": "e49f9d55-801d-4266-b3ec-565156f97955"
   },
   "outputs": [],
   "source": [
    "rating_counts = ratings.loc[:,\"user_id\"].value_counts()\n",
    "item_counts = ratings.loc[:,\"user_id\"].value_counts()\n",
    "\n",
    "_, ax = plt.subplots(1,2, figsize=(10,4));\n",
    "ax[0].hist(rating_counts, bins=100);\n",
    "ax[0].set_title('Histogram of number of ratings for users');\n",
    "ax[0].set_xlabel('Number of ratings');\n",
    "ax[0].set_ylabel('Count');\n",
    "\n",
    "ax[1].hist(item_counts, bins=100);\n",
    "ax[1].set_title('Histogram of number of ratings for items');\n",
    "ax[1].set_xlabel('Number of ratings');\n",
    "ax[1].set_ylabel('Count');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee75e1-2de5-4ae4-82cb-2a151aa5356d",
   "metadata": {
    "id": "97ee75e1-2de5-4ae4-82cb-2a151aa5356d"
   },
   "source": [
    "The number of ratings by both users and items shows an exponential or power-law distribution. Most users and items have very few ratings. However, some items and ratings have a great many ratings.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2424ae9c-2b03-4532-b3a7-4088105d09a0",
   "metadata": {
    "id": "2424ae9c-2b03-4532-b3a7-4088105d09a0"
   },
   "source": [
    "## Creating Sparse Train and Test Matrices  \n",
    "\n",
    "The **utility matrix** has high dimensions and is extremely sparse. This fact can be understood by considering the small number of reviews compared to the dimensions of the matrix. In this case, we have the users in the rows and the items in the columns. Given the high dimensionality and sparsity of the utility matrix representing this data structure as a full dense matrix with missing values is undesireable. In fact, for massive scale recommenders there is no reasonable cluster with sufficient memory accommodate such a data structure. Fortunately, there is an alternative, use a **hash table**. The non-missing entries of the utility matrix are then stored as key-value pairs. The key for the sparse matrix data structure is the row-column tuple, which is hashed for reference to the value.     \n",
    "\n",
    "The code in the cell below has two major steps:    \n",
    "1. The data frame containing the user reviews is converted to a [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) matrix.\n",
    "2. The sparse matrix is sampled into train and test subsets. The sampling is done randomly by rating. We do not want to simply sample by users or items as we need to be able to train and test on ratings.\n",
    "\n",
    "Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525880a-1ed3-44b1-8418-24fa2fb49383",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f525880a-1ed3-44b1-8418-24fa2fb49383",
    "outputId": "30a1899d-79f2-46d9-a9bd-c1c9ef7684c7"
   },
   "outputs": [],
   "source": [
    "def create_spare_matrix(matrix):\n",
    "  \"\"\"\n",
    "  Function to create a sparse csr_matrix from a dataframe of user ratings\n",
    "\n",
    "  Args: matrix - a data frame with the user-item ratings in the rows\n",
    "\n",
    "  Returns: a sparse csr_matrix\n",
    "  \"\"\"\n",
    "  # Adjust user and item IDs to be zero-based (important for sparse matrix indexing)\n",
    "  matrix['user_id'] -= 1\n",
    "  matrix['item_id'] -= 1\n",
    "\n",
    "  # Create sparse matrix: rows = users, columns = items, values = ratings\n",
    "  num_users = matrix['user_id'].max() + 1\n",
    "  num_items = matrix['item_id'].max() + 1\n",
    "\n",
    "  sparse = csr_matrix(\n",
    "    (matrix['rating'], (matrix['user_id'], matrix['item_id'])),\n",
    "    shape=(num_users, num_items)\n",
    "  )\n",
    "  return sparse\n",
    "\n",
    "def train_test_split_sparse(matrix, test_fraction=0.2, random_state=None):\n",
    "    # Create a sparse matrix\n",
    "    sparse = create_spare_matrix(matrix)\n",
    "\n",
    "    # Convert to COO format for easy indexing\n",
    "    sparse_coo = sparse.tocoo()\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Get all non-zero indices\n",
    "    n_samples = sparse_coo.nnz\n",
    "    indices = np.arange(n_samples)\n",
    "    test_size = int(n_samples * test_fraction)\n",
    "\n",
    "    # Randomly select test indices\n",
    "    test_idx = rng.choice(indices, size=test_size, replace=False)\n",
    "    train_idx = np.setdiff1d(indices, test_idx)\n",
    "\n",
    "    # Build train and test sparse matrices\n",
    "    train = csr_matrix((sparse_coo.data[train_idx],\n",
    "                        (sparse_coo.row[train_idx], sparse_coo.col[train_idx])),\n",
    "                       shape=sparse_coo.shape)\n",
    "\n",
    "    test = csr_matrix((sparse_coo.data[test_idx],\n",
    "                       (sparse_coo.row[test_idx], sparse_coo.col[test_idx])),\n",
    "                      shape=sparse_coo.shape)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "sparse_ratings_train, sparse_ratings_test = train_test_split_sparse(ratings, test_fraction=0.2, random_state=543)\n",
    "\n",
    "print(f\"Number of training users: {sparse_ratings_train.shape[0]}\")\n",
    "print(f\"Number of training items: {sparse_ratings_train.shape[1]}\")\n",
    "print(f\"Number of non-zero entries in sparse train matrix: {sparse_ratings_train.nnz}\")\n",
    "print(f\"\\nNumber of test users: {sparse_ratings_test.shape[0]}\")\n",
    "print(f\"Number of test items: {sparse_ratings_test.shape[1]}\")\n",
    "print(f\"Number of non-zero entries in sparse test matrix: {sparse_ratings_test.nnz}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a35fb5-a09e-4d2d-ad5f-d3334db73c6a",
   "metadata": {
    "id": "c9a35fb5-a09e-4d2d-ad5f-d3334db73c6a"
   },
   "source": [
    "> **Exercise 6-01:** The question one should think about is how much memory does the sparse matrix (hash table) representation save? We can quantify this by the compression ratio:\n",
    "> $$compression\\ ratio = \\frac{size\\ of\\ sparse\\ matrix}{size\\ of\\ dense\\ matrix}$$\n",
    "> You will now work this out for two cases. Assume that each number requires 8 bytes of memory for the sparse representation. Keep in mind that the key is actually a two value tuple, $(user_ID, item_ID$. We can ignore the small overhead of the row and column indices for the dense representation. Assume all 100,000 ratings are in one matrix.    \n",
    "> 1. In the cell below create code to compute a) the size of the full matrix, b) the size of the sparse matrix, and c) the compression achieved using the sparse representation.          \n",
    "> 2. Next perform the same calculations for a more realistic size systems with 1 million items and 100 million users and an average of 20 ratings per user.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2d89a-6863-4e11-9359-9e0f30dc7816",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32c2d89a-6863-4e11-9359-9e0f30dc7816",
    "outputId": "49433b12-379b-49ee-86b4-6d8b03da298d"
   },
   "outputs": [],
   "source": [
    "## Put your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0125069-af10-4e56-ac36-79d655b1d814",
   "metadata": {
    "id": "e0125069-af10-4e56-ac36-79d655b1d814"
   },
   "source": [
    "> **End of Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd17bbb1-b8c5-4f28-ba38-672ec457747f",
   "metadata": {
    "id": "cd17bbb1-b8c5-4f28-ba38-672ec457747f"
   },
   "source": [
    "The next step is to filter the train and test datasets to remove items or users with too few ratings. Having items or users with too few ratings increases the error of rating predictions and can lead to an over-fit model. Execute the code in the cell below to apply these filters.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6f8c5-583e-436e-a58e-443ac3155727",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fa6f8c5-583e-436e-a58e-443ac3155727",
    "outputId": "57ff5cb7-870e-4139-c290-56e0c848e5d5"
   },
   "outputs": [],
   "source": [
    "def filter_users(ratings_train, ratings_test, min_rating_count):\n",
    "  # Get the count of nonzero values in rows\n",
    "  row_counts_train = ratings_train.count_nonzero(axis=1)\n",
    "  row_counts_test = ratings_test.count_nonzero(axis=1)\n",
    "\n",
    "  # Create and apply a boolean mask for rows with enough nonzeros\n",
    "  mask = (row_counts_train >= min_rating_count) & (row_counts_test >= min_rating_count)\n",
    "  return ratings_train[mask, :], ratings_test[mask,:]\n",
    "\n",
    "def filter_items(ratings_train, ratings_test, min_rating_count):\n",
    "  # Get the count of nonzero values in rows\n",
    "  col_counts_train = ratings_train.count_nonzero(axis=0)\n",
    "  col_counts_test = ratings_test.count_nonzero(axis=0)\n",
    "\n",
    "  # Create and apply a boolean mask for rows with enough nonzeros\n",
    "  mask = (col_counts_train >= min_rating_count) & (col_counts_test >= min_rating_count)\n",
    "  return ratings_train[:, mask], ratings_test[:, mask]\n",
    "\n",
    "\n",
    "min_rating_count = 5\n",
    "sparse_ratings_train, sparse_ratings_test = filter_users(sparse_ratings_train, sparse_ratings_test, min_rating_count)\n",
    "sparse_ratings_train, sparse_ratings_test = filter_items(sparse_ratings_train, sparse_ratings_test, min_rating_count)\n",
    "\n",
    "print(f\"Number of training users: {sparse_ratings_train.shape[0]}\")\n",
    "print(f\"Number of training items: {sparse_ratings_train.shape[1]}\")\n",
    "print(f\"Number of non-zero entries in sparse train matrix: {sparse_ratings_train.nnz}\")\n",
    "print(f\"\\nNumber of test users: {sparse_ratings_test.shape[0]}\")\n",
    "print(f\"Number of test items: {sparse_ratings_test.shape[1]}\")\n",
    "print(f\"Number of non-zero entries in sparse test matrix: {sparse_ratings_test.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39279c7d-825b-40b3-a57b-cc3193e1a3dd",
   "metadata": {
    "id": "39279c7d-825b-40b3-a57b-cc3193e1a3dd"
   },
   "source": [
    "> **Exercise 6-02:** Consider the effects that these filtering operations have on the results expected from any recommender system and answer these questions.\n",
    "> 1. How will this filtering impact the ability of the recommender to show items in the 'long tail'?\n",
    "> 2. How will this filtering impact the user cold start problem for the recommender?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f944b0-9c9d-47b1-9c1e-468a1e09b55c",
   "metadata": {
    "id": "d0f944b0-9c9d-47b1-9c1e-468a1e09b55c"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.          \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345fd5b-d06d-4c65-9a85-09fe0226a807",
   "metadata": {
    "id": "b345fd5b-d06d-4c65-9a85-09fe0226a807"
   },
   "source": [
    "## Create a Baseline Model\n",
    "\n",
    "A baseline recommender model imputes the mean rating for each item. The mean implied rating is the sum of the mean rating for a user and the mean rating for the item. For user $x$ and item $i$ the implied baseline rating is:  \n",
    "$$\\hat{r}_{x,i} = \\mu + \\bar{r}_x + \\bar{r}_i$$     \n",
    "where:  \n",
    "$\\mu$ is the overall mean rating for all users and items.     \n",
    "$\\bar{r}_x$ is the mean rating of items by user $x$.    \n",
    "$\\bar{r}_i$ is the mean rating of item, $i$, for all users rating that item.     \n",
    "\n",
    "The ranks of these mean or expected implied ratings are the recommendations. In simple terms the baseline model uses 'crowd sourced' mean ratings.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60041910-3a4b-4c84-88cf-10ca3045e126",
   "metadata": {
    "id": "60041910-3a4b-4c84-88cf-10ca3045e126"
   },
   "source": [
    "### Compute the user mean ratings\n",
    "\n",
    "The first step of computing a baseline is to zero-mean the overall ratings along with the ratings of each user and item. This step removes biases from users with exceptionally high or low average ratings. Execute the code in the cell below to perform these operations.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854a6f8-3197-42d3-bce4-801898eb30cf",
   "metadata": {
    "id": "4854a6f8-3197-42d3-bce4-801898eb30cf"
   },
   "outputs": [],
   "source": [
    "def indices_to_rows(sparse):\n",
    "    \"\"\"Returns row index for each data entry in CSR matrix.\"\"\"\n",
    "    row_indices = np.empty_like(sparse.data, dtype=int)\n",
    "    for i in range(len(sparse.indptr) - 1):\n",
    "        row_indices[sparse.indptr[i]:sparse.indptr[i+1]] = i\n",
    "    return row_indices\n",
    "\n",
    "def compute_row_means(X):\n",
    "    row_sums = X.sum(axis=1).A1  # Convert to 1D array\n",
    "    row_counts = np.diff(X.indptr)\n",
    "    row_counts[row_counts < 1] = 1\n",
    "    return row_sums / row_counts\n",
    "\n",
    "def compute_column_means(X):\n",
    "    col_sums = X.sum(axis=0).A1\n",
    "    col_counts = np.diff(X.tocsc().indptr)\n",
    "    col_counts[col_counts < 1] = 1\n",
    "    return col_sums / col_counts\n",
    "\n",
    "\n",
    "\n",
    "def zero_center_sparse(X):\n",
    "    \"\"\"\n",
    "    Computes and returns the Zero-centers sparse matrix by rows and columns,\n",
    "    preserving sparsity along with the means of the rows and columns.\n",
    "\n",
    "    Args:\n",
    "        sparse_matrix (scipy.sparse.csr_matrix): The input sparse matrix.\n",
    "        axis: the axis along which to center the values\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: The zero-centered sparse matrix.\n",
    "        Mean: the overall mean rating for all items and users.\n",
    "        User_means: a vector of the user or row means\n",
    "        item_means: a vector of the item or column means\n",
    "    \"\"\"\n",
    "    if not isinstance(X, csr_matrix):\n",
    "        X = X.tocsr()\n",
    "\n",
    "    # Subtract overall mean from non-zero entries\n",
    "    sum_ratings = X.sum()\n",
    "    mean_rating = sum_ratings / X.nnz\n",
    "    X_new = X.copy().astype(np.float32)\n",
    "    X_new.data -= mean_rating\n",
    "\n",
    "    # Compute row means\n",
    "    row_means = compute_row_means(X_new)\n",
    "\n",
    "    # Compute column means\n",
    "    col_means = compute_column_means(X_new)\n",
    "\n",
    "    # Subtract row means from non-zero entries\n",
    "    X_new.data -= row_means[indices_to_rows(X_new)]\n",
    "\n",
    "\n",
    "    # Subtract column means from non-zero entries\n",
    "    X_new.data -= col_means[X_new.indices]\n",
    "\n",
    "    return X_new, mean_rating, row_means, col_means\n",
    "\n",
    "centered_spare_ratings_train, mean_rating, user_means, item_means = zero_center_sparse(sparse_ratings_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e772d98-378c-4668-87fe-1e4a89d0163b",
   "metadata": {
    "id": "6e772d98-378c-4668-87fe-1e4a89d0163b"
   },
   "source": [
    "With the means and centered sparse utility matrix computed, let's explore the result. Execute the code in the cell below to display the mean rating along with histograms of the zero-centered rating, the centered user ratings and the centered item ratings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757095c-31d1-4ee6-850e-ae7717edc8b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5757095c-31d1-4ee6-850e-ae7717edc8b2",
    "outputId": "7b4647f8-a11c-4a16-cfa6-b501e327d11e"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean rating: {mean_rating}\")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(centered_spare_ratings_train.data, bins=50);\n",
    "plt.title('Distribution zero-centered ratings')\n",
    "plt.xlabel('rating');\n",
    "plt.ylabel('Count');\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(user_means, bins=50);\n",
    "plt.title('Distribution of user means')\n",
    "plt.xlabel('rating');\n",
    "plt.ylabel('Count');\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(item_means, bins=50);\n",
    "plt.title('Distribution of item means')\n",
    "plt.xlabel('rating');\n",
    "plt.ylabel('Count');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c243b-d29e-4a8f-bcf3-e113d48906ec",
   "metadata": {
    "id": "4e0c243b-d29e-4a8f-bcf3-e113d48906ec"
   },
   "source": [
    "> **Exercise 6-03:** Examine the foregoing histograms and answer these questions.\n",
    "> 1. Notice that the bulk of the zero-centered ratings are in the range of about $(-2,2)$. What does this range of variation tell you about the overall ratings?\n",
    "> 2. Notice that the bulk of the user mean ratings are in a narrow range of about $(-0.8,0.8)$ and the mean item ratings are in a narrow range of about $(-1.3,0.7)$. What do these results tell you about the variation in ratings from item to item and user to user?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6341b58-8b40-4599-a9ad-92d6e65ca777",
   "metadata": {
    "id": "c6341b58-8b40-4599-a9ad-92d6e65ca777"
   },
   "source": [
    "> **Answers:**   \n",
    "> 1.         \n",
    "> 2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3f4cb-9752-4b1e-8ef1-b3e39168e16e",
   "metadata": {
    "id": "58f3f4cb-9752-4b1e-8ef1-b3e39168e16e"
   },
   "source": [
    "### Compute the Baseline Rating Predictions   \n",
    "\n",
    "With the mean ratings for items and users computed it is time to compute the baseline rating predictions. Conceptually, this is a simple matter of adding the means for each user_ID, item_ID tuple. Execute the code in the cell below to do just this and return the baseline implied ratings.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4922a-3266-41a9-9c24-b269ba332195",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "10b4922a-3266-41a9-9c24-b269ba332195",
    "outputId": "207d9abf-94dd-4908-c307-dbbb5f0cb5d4"
   },
   "outputs": [],
   "source": [
    "def baseline_rating_predict(rating_mean, means_user, means_item, sparse_ratings_test):\n",
    "  # Create a new sparse matrix with zero values\n",
    "  shape = sparse_ratings_test.shape\n",
    "\n",
    "  # Get the indptr and indices arrays from the template matrix\n",
    "  indptr = sparse_ratings_test.indptr\n",
    "  indices = sparse_ratings_test.indices\n",
    "\n",
    "  # Create a new empty sparse matrix\n",
    "  data = csr_matrix((shape), dtype=sparse_ratings_test.dtype)\n",
    "\n",
    "  ## Find the row, column and index\n",
    "  rows, cols = sparse_ratings_test.nonzero()\n",
    "\n",
    "  ## Add the row and column means\n",
    "  means_temp_user = means_user + rating_mean\n",
    "  data = means_item[cols] + means_temp_user[rows]\n",
    "\n",
    "  return csr_matrix((data, indices, indptr), shape=shape)\n",
    "\n",
    "predicted_baseline_ratings = baseline_rating_predict(mean_rating, user_means, item_means, sparse_ratings_test)\n",
    "predicted_baseline_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e03eb-b7a1-41c6-85b4-ec842be1126a",
   "metadata": {
    "id": "794e03eb-b7a1-41c6-85b4-ec842be1126a"
   },
   "source": [
    "### Evaluate the Baseline Predicted Ratings  \n",
    "\n",
    "With the baseline implied ratings computed we need to evaluate the results. The code in the cell below does the following:     \n",
    "1. The difference between the actual rating and the implied rating is computed.\n",
    "2. Some basic error statistics are computed.\n",
    "3. A plot of the errors vs. the actual rating is displayed.\n",
    "\n",
    "Execute the code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9962fc8c-0eaf-481b-96c4-297b66f73831",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "9962fc8c-0eaf-481b-96c4-297b66f73831",
    "outputId": "e23739f9-fe1b-4d70-f388-ac3212ed81cc"
   },
   "outputs": [],
   "source": [
    "def evaluate_recommender(ratings_test, predicted_ratings):\n",
    "\n",
    "  ## Compute the sparse error array\n",
    "  ratings_errors = predicted_ratings - ratings_test\n",
    "\n",
    "  ## Create data frame from the sparse test array\n",
    "  ratings_test_coo = ratings_test.tocoo()\n",
    "  rating_test_df = df = pd.DataFrame({\n",
    "    'row_index': ratings_test_coo.row,\n",
    "    'col_index': ratings_test_coo.col,\n",
    "    'Rating': ratings_test_coo.data\n",
    "  })\n",
    "\n",
    "  ## Create data frame from the sparse error array\n",
    "  ratings_errors_coo = ratings_errors.tocoo()\n",
    "  rating_errors_df = df = pd.DataFrame({\n",
    "    'row_index': ratings_errors_coo.row,\n",
    "    'col_index': ratings_errors_coo.col,\n",
    "    'Rating_Error': ratings_errors_coo.data\n",
    "  })\n",
    "  rating_errors_df.loc[:,'Rating'] = rating_test_df.loc[:,'Rating']\n",
    "\n",
    "  ## Print some error statistics\n",
    "  print(f\"Mean error: {round(rating_errors_df.loc[:,'Rating_Error'].mean(),3)}\")\n",
    "  print(f\"Root mean square error: {round(rating_errors_df.loc[:,'Rating_Error'].std(),3)}\")\n",
    "  print(f\"Median absolute error: {round(rating_errors_df.loc[:,'Rating_Error'].abs().median(),3)}\\n\")\n",
    "\n",
    "  ## PLot the error distributions\n",
    "  g = sns.jointplot(data=rating_errors_df, x='Rating', y='Rating_Error',\n",
    "                  kind='hex', height=6, ratio=4);\n",
    "  sns.regplot(data=rating_errors_df,\n",
    "            x='Rating',\n",
    "            y='Rating_Error',\n",
    "            ax=g.ax_joint,\n",
    "            scatter=False,\n",
    "            color='red',\n",
    "            line_kws={'linestyle': '--', \"linewidth\": 1},\n",
    "            ci=95);\n",
    "  plt.suptitle(\"Rating error vs. rating\\n Regression line in red\", y=1.02);\n",
    "\n",
    "evaluate_recommender(sparse_ratings_test, predicted_baseline_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee083684-72f0-47c7-b30f-2c5de39e6f7f",
   "metadata": {
    "id": "ee083684-72f0-47c7-b30f-2c5de39e6f7f"
   },
   "source": [
    "> **Exercise 6-04:** Examine the plot of the error vs the actual rating. Note the systematic bias that changes with actual rating. Given the baseline model, how can you explain this effect?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accd52d-84fc-4a96-856b-814f4530f77c",
   "metadata": {
    "id": "4accd52d-84fc-4a96-856b-814f4530f77c"
   },
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94480608-3afa-4f61-9312-74c333b08e46",
   "metadata": {
    "id": "94480608-3afa-4f61-9312-74c333b08e46"
   },
   "source": [
    "## K Nearest Neighbor Search Collaborative Filtering  \n",
    "\n",
    "**Collaborative filtering** algorithms search for the highest similarity between behavior or engagement between items or between users.\n",
    "For this algorithm we will apply a simple **k-nearest-neighbor search** algorithm. In this case we will do user-user nearest neighbor search. The search is performed between a user's recommendation profile, the query, $q$, and the recommendation profiles of the other users, $u$. The $k$ nearest neighbor ratings to the query are used to compute the implied ratings. The similarities, $s(r_q, r_u)$, are used as weights to compute the mean rating, $\\hat{r}_i$, for each item, $i$.\n",
    "$$\\hat{r}_i = \\frac{\\sum^k_{u=1} s(r_{q,i}, r_{u,i}) \\times r_{u,i}}{\\sum^k_{u=1} s(r_{q,i}, r_{u,i})}$$\n",
    "\n",
    "The major steps in this algorithm are:      \n",
    "1. The query rating profile vector and profile vectors of the other users are zero-centered and variance normalized. This allows **cosine similarity** to be computed using dot products.\n",
    "2. The cosine similarity between the query and the other user's normalized vectors is computed.\n",
    "3. The similarities are ranked, or sorted in descending order, and the top $k$ are retained.\n",
    "4. The implied rating is computed as the weighted average of the ratings of the top $k$ users.\n",
    "5. The implied ratings for the query user are sorted in descending order and the top few are retained.\n",
    "6. The items with the highest implied ratings are the recommendations.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6c73a-ef91-4e3c-b354-4e2b575c672f",
   "metadata": {
    "id": "b883802e-0ee0-4506-b4d6-d1c50ec3aa29"
   },
   "source": [
    "> **Exercise 6-05:** In one or a few sentences explain the key difference between the learning in content-based recommender algorithms and collaborative filtering recommender algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156edcc5-75d7-4463-ae8d-c2f62befc7b9",
   "metadata": {
    "id": "091b1856-014b-4396-af5b-e5e8bbc2a118"
   },
   "source": [
    "> **Answer:**       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22af5321-f945-4d48-b130-e74131f125d6",
   "metadata": {},
   "source": [
    "\n",
    "The code in the cell below computes the $k$ nearest neighbor users to the query user, returning the neighbor user ids and the similarities. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee87f7-cbaf-4783-b29d-b0ad448b4947",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a9ee87f7-cbaf-4783-b29d-b0ad448b4947",
    "outputId": "727099e8-71c8-4bca-f73d-1e4fb481f98a"
   },
   "outputs": [],
   "source": [
    "def normalize_ratings(ratings):\n",
    "    \"\"\"\n",
    "    Function returns the normalized user ratings (rows) of the\n",
    "    sparse utility matrix.\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = ratings.shape\n",
    "    X = ratings.copy()  # avoid modifying original\n",
    "\n",
    "    new_data = []\n",
    "    new_indices = []\n",
    "    new_indptr = [0]\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        row_start, row_end = X.indptr[i], X.indptr[i + 1]\n",
    "        indices = X.indices[row_start:row_end]\n",
    "        data = X.data[row_start:row_end]\n",
    "\n",
    "        nnz = len(data)\n",
    "        mean = data.sum() / nnz\n",
    "        norm = np.sqrt(np.sum((data - mean) ** 2))\n",
    "        #var = sq_sum / nnz\n",
    "\n",
    "        new_data.extend((data - mean) / norm)\n",
    "\n",
    "        new_indices.extend(indices)\n",
    "        new_indptr.append(len(new_data))\n",
    "\n",
    "    return csr_matrix((new_data, new_indices, new_indptr), shape=X.shape)\n",
    "\n",
    "\n",
    "def normalize_query(query):\n",
    "    \"\"\"\n",
    "    Function to compute zero mean unit variance query vector\n",
    "    \"\"\"\n",
    "    nnz = query.nnz\n",
    "    data = query.data\n",
    "\n",
    "    # zero mean the data\n",
    "    mean = data.sum() / nnz\n",
    "    data = data - mean\n",
    "\n",
    "    # Variance\n",
    "    #var = np.sum((data) ** 2) / nnz\n",
    "    norm = np.sqrt(np.sum((data) ** 2))\n",
    "\n",
    "    # normalize by standard deviaton\n",
    "    data = data / norm\n",
    "    return csr_matrix((data, query.indices, query.indptr), shape=query.shape)\n",
    "\n",
    "\n",
    "def user_cosine_similarity(ratings, query, k=20):\n",
    "  # Variance normalize the vectors\n",
    "  ratings_normalized = normalize_ratings(ratings)\n",
    "  query_normalized = normalize_query(query)\n",
    "\n",
    "  rating_similarity = ratings_normalized.dot(query_normalized.T).toarray().flatten()\n",
    "  out = pd.DataFrame({'user':range(ratings.shape[0]), 'similarity':rating_similarity})\n",
    "  return out.sort_values('similarity', ascending=False)[:k]\n",
    "\n",
    "similarity_query = user_cosine_similarity(centered_spare_ratings_train, sparse_ratings_test[0,:], k=50)\n",
    "similarity_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6cf1d8-24e7-4bb2-a156-df4e92e6add9",
   "metadata": {
    "id": "ef6cf1d8-24e7-4bb2-a156-df4e92e6add9"
   },
   "source": [
    "With the similarities for the top $k$ nearest neighbors computed, the next step is to compute the implied ratings as the similarity weighted average of the ratings. One needs to be careful to only include the non-zero elements of each column (item) vector when making this calculation. Execute the code in the cell below to compute the implied ratings for all items.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e43753-8385-4b96-94ba-61324f64fee1",
   "metadata": {
    "id": "a3e43753-8385-4b96-94ba-61324f64fee1"
   },
   "outputs": [],
   "source": [
    "def knn_implied_ratings(ratings, similarities, item_ratings):\n",
    "  mask = [True if i in similarities.user else False for i in range(ratings.shape[0])]\n",
    "  X = ratings[mask,:].copy()\n",
    "  weighted_rating = np.array([0.0]*X.shape[1])\n",
    "\n",
    "  for j in range(X.shape[1]):\n",
    "    col = X[:, j]\n",
    "    rows = col.nonzero()[0]\n",
    "    row_mask = [True if k in rows else False for k in range(X.shape[0])]\n",
    "    col_weight = np.sum(similarities.loc[row_mask, 'similarity'])\n",
    "    if col_weight > 0:\n",
    "        weighted_rating[j] = np.dot(col.toarray().flatten().T, similarities.loc[:, 'similarity'])/col_weight\n",
    "    else:\n",
    "        weighted_rating[j] = item_ratings[j]\n",
    "  return  weighted_rating\n",
    "\n",
    "implied_ratings = knn_implied_ratings(centered_spare_ratings_train, similarity_query, item_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c849412-a317-429f-a259-934f2dc92174",
   "metadata": {
    "id": "8c849412-a317-429f-a259-934f2dc92174"
   },
   "source": [
    "As a check on the implied rating calculation execute the code in the cell below to display a histogram. Keep in mind that we are still working with zero-centered rating values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a1eab-b397-4ca7-b611-3421e1ac4452",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "c31a1eab-b397-4ca7-b611-3421e1ac4452",
    "outputId": "907cc10d-1a83-4cf6-d267-5b6790f01f2c"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(implied_ratings, bins=30);\n",
    "plt.title('Distribution zero-centered ratings')\n",
    "plt.xlabel('rating');\n",
    "plt.ylabel('Count');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9041cad-3a6a-4c9b-965c-f2bcaa1792d1",
   "metadata": {
    "id": "e9041cad-3a6a-4c9b-965c-f2bcaa1792d1"
   },
   "source": [
    "Finally, we are ready to compute the recommendations. To insure that recommendations are generated only for items the query user has not already rated, a mask is created. The implied ratings are sorted in descending order and the list truncated, which is the recommendation list. Execute the code in the cell below and examine the result.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc90ee9-8c22-44ed-9a77-1fa4bdacdcd6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "6dc90ee9-8c22-44ed-9a77-1fa4bdacdcd6",
    "outputId": "e783dd3b-561c-4690-8fdd-74ba9b69c2c7"
   },
   "outputs": [],
   "source": [
    "def knn_recommender(query, ratings, k):\n",
    "  rated_items = query.nonzero()[1]\n",
    "  mask = [False if i in rated_items else True for i in range(query.shape[1])]\n",
    "  # mask = np.where(ratings[user_id, :].toarray()[0] == 0)[0]\n",
    "  unrated = ratings[mask]\n",
    "  out = pd.DataFrame({'item_index':range(len(unrated)), 'ratings': unrated})\n",
    "  return out.sort_values('ratings', ascending=False)[:k]\n",
    "\n",
    "\n",
    "knn_recommendations = knn_recommender(sparse_ratings_test[0,:], implied_ratings, k=10)\n",
    "knn_recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf1b76-2bbb-4f5e-a1d7-1cba472d4eb3",
   "metadata": {
    "id": "6baf1b76-2bbb-4f5e-a1d7-1cba472d4eb3"
   },
   "source": [
    "To make this recommendation list human-understandable, it is easy to find the movie titles, using the `item_id`. Execute the code in the cell below and examine the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893b3d9-0943-45a4-9a34-8ea1d240b2a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "5893b3d9-0943-45a4-9a34-8ea1d240b2a1",
    "outputId": "497a24d8-eb1f-438b-fae3-e487608c21ab"
   },
   "outputs": [],
   "source": [
    "movies.loc[knn_recommendations['item_index'],'movie title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f37c5f-f1e7-4add-ae45-986bb16b5dce",
   "metadata": {
    "id": "33f37c5f-f1e7-4add-ae45-986bb16b5dce"
   },
   "source": [
    "> **Exercise 6-06:** We will not take the time to do a full evaluation of the kNN collaborative filtering algorithm. You can at this point make a few observations of the outcome for the single query and answer these questions.\n",
    "> 1. One possible evaluation of the kNN collaborative filtering algorithm is to compare the distribution of the implied (estimated) ratings with the distribution of the actual zero-centered item ratings. Compare the histogram displayed above to the histogram of the zero-centered item ratings display previously. Do these two distributions appear to be approximately the same or not and what are some noticeable differences, if any? What does this tell you about the accuracy of this algorithm?\n",
    "> 2. In a few sentences, explain why the simple kNN collaborative filtering algorithm used for the foregoing example is not scaleable. Try to be as specific as you can.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc77367-4d7f-4f61-b1ea-6664d1a9f273",
   "metadata": {
    "id": "ddc77367-4d7f-4f61-b1ea-6664d1a9f273"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.      \n",
    "> 2.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e8a2d-ef37-4815-87f6-8b2bc75bf667",
   "metadata": {
    "id": "bb3e8a2d-ef37-4815-87f6-8b2bc75bf667"
   },
   "source": [
    "## Collaborative Filtering by Matrix Decomposition    \n",
    "\n",
    "Given the limited scalability of the basic kNN search algorithm, we need to look elsewhere. The solution is to apply **latent variable** models. These models use various techniques **dimensionality reduction** to compress the high-dimensional utility matrix space to a lower dimensional **latent space**. There are two general approaches to learning latent spaces:   \n",
    "1. **Unsupervised learning** algorithms which compute a **factorization** of the utility matrix. For this example we will use [**singular value decomposition (SVD)**](https://en.wikipedia.org/wiki/Singular_value_decomposition) to learn a set of orthogonal latent variables. The orthogonalization is a linear process.        \n",
    "2. **Supervised learning** algorithms compute optimal weights for a set of latent variables by reducing a loss function. There are quite a number of algorithms that have been applied to this problem, mostly of which use some form of neural networks. These algorithms are able to learn interactions between variables. We will not purse this approach further here.\n",
    "\n",
    "### Matrix Factorization    \n",
    "\n",
    "We can factorize the utility matrix using singular value decomposition. The implied rating matrix, $\\hat{R}$, can then be computed using the factorization. This is done by matrix multiplication:     \n",
    "$$\\hat{R} = U \\Sigma V^T$$    \n",
    "Where for $X$ users and $I$ items,     \n",
    "$R \\in \\{X\\ \\times I \\}$ is the utility matrix containing ratings.  \n",
    "$U \\in \\{I \\times s \\}$ is the matrix of left singular vectors.          \n",
    "$\\Sigma \\in \\{s \\times s\\}$ is the diagonal matrix of singular values with $s \\le min(X, I)$.       \n",
    "$V \\in \\{X \\times s\\}$ is the matrix of right singular vectors.  \n",
    "\n",
    "By selecting a value of $s \\le min(X, I)$ the SVD reduces dimensionality. The $s \\times s$ diagonal matrix of singular values are the weights or **loadings** of the factors. The singular vector matrices contain the orthogonal bases for the factors.\n",
    "\n",
    "The code in the cell below uses the [scipy.sparse.linalg.svds](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.svds.html) function to perform the SVD decomposition of the sparse utility matrix. Since this algorithm explicitly works with sparse matrices the bias from zero padding of missing values. Execute the code in the cell below to perform the SVD on the centered sparse utility matrix and displays a plot of the singular values.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f997da-b59d-4721-ab36-46c7db1f7a09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "f0f997da-b59d-4721-ab36-46c7db1f7a09",
    "outputId": "4371650f-81da-40af-bfb9-5e82f636b9df"
   },
   "outputs": [],
   "source": [
    "k=100\n",
    "U, sigma, Vt = svds(centered_spare_ratings_train, k=k)\n",
    "\n",
    "\n",
    "print(f\"Shape of U: {U.shape}\")\n",
    "print(f\"Shape of V transpose: {Vt.shape} \\n\\n\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "x = [i+1 for i in range(len(sigma))]\n",
    "plt.plot(x, sigma[::-1]);\n",
    "plt.title('Singular values from utility matrix decomposition')\n",
    "plt.xlabel('Index');\n",
    "plt.ylabel('Singular value');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189be718-ca8f-41d0-b6b4-4ff055c037c2",
   "metadata": {
    "id": "189be718-ca8f-41d0-b6b4-4ff055c037c2"
   },
   "source": [
    "> **Exercise 6-07:** Notice that the graph of singular values drops rapidly at first and then at a slower steady rate. Consider the effect of increasing or decreasing $s$ the number of singular values retained.\n",
    "> 1. How does the accuracy of the decomposition in representing the full matrix change as $s$ increases? How could choosing a value of S that is too large lead to an over-fit model?\n",
    "> 2. Roughly how does the number of operations required to compute the implied ratings increase with $s$.\n",
    "> 3. If we use 8 byte floating point numbers to represent $U$ and $V$ how big is the data structure in this cases. How does the memory required scale as we increase s?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9145080-443c-4429-978b-e9a7f84e05e2",
   "metadata": {
    "id": "a9145080-443c-4429-978b-e9a7f84e05e2"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.         \n",
    "> 2.         \n",
    "> 3.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c38c54-2e98-404a-9442-233892ae83ca",
   "metadata": {
    "id": "f7c38c54-2e98-404a-9442-233892ae83ca"
   },
   "source": [
    "With the utility matrix factorized, we are ready to compute the implied ratings. Execute the code in the cell below to compute the implied ratings from the utility matrix decomposition.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0772a-4580-4580-82a2-b5fb20f50d94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "bac0772a-4580-4580-82a2-b5fb20f50d94",
    "outputId": "bac32f2d-b1d9-490e-ce8d-056cafa5a5fe"
   },
   "outputs": [],
   "source": [
    "def svd_predict_ratings(U, Vt, sigma, rating_mean, means_items, means_user):\n",
    "    \"\"\"\n",
    "    Predicts ratings using matrix factorization, adding back user and item means.\n",
    "\n",
    "    Args:\n",
    "        U (np.ndarray): User latent factor matrix.\n",
    "        V (np.ndarray): Item latent factor matrix.\n",
    "        means_items (np.ndarray): Vector of item means.\n",
    "        means_user (np.ndarray): Vector of user means.\n",
    "        sparse_ratings_test (scipy.sparse.csr_matrix): The sparse matrix of test ratings.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: Sparse matrix of predicted ratings.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of U and V.T\n",
    "    sigma_diag = np.diag(sigma)\n",
    "    sparse_predictions = csr_matrix(np.dot(np.dot(U, sigma_diag), Vt))\n",
    "\n",
    "    ## Find the row, column and index\n",
    "    rows, cols = sparse_predictions.nonzero()\n",
    "\n",
    "    return sparse_predictions\n",
    "\n",
    "\n",
    "SVD_rating_prediction = svd_predict_ratings(U, Vt, sigma, mean_rating, item_means, user_means)\n",
    "evaluate_recommender(sparse_ratings_test, SVD_rating_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa564d-f2df-48b1-9d09-40067f254d9f",
   "metadata": {
    "id": "3daa564d-f2df-48b1-9d09-40067f254d9f"
   },
   "source": [
    "> **Exercise 6-08:** Compare the evaluation of the matrix factorization recommender with the baseline recommender you evaluated previously and answer these questions.\n",
    "> 1. How does the accuracy and bias compare?\n",
    "> 2. What do the accuracy and bias tell you about the adequacy of the SVD factorization as a recommender representation?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27a39a-625c-4e5a-b950-b7ee6d2cfe82",
   "metadata": {
    "id": "df27a39a-625c-4e5a-b950-b7ee6d2cfe82"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.       \n",
    "> 2.          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7068a0d-0252-4866-936c-be565433a556",
   "metadata": {
    "id": "d7068a0d-0252-4866-936c-be565433a556"
   },
   "source": [
    "There is one final step, computing the recommendations. The code in the cell below computes the implied (predicted) ratings for a query user. The ratings of unrated items are sorted in descending order and the item ids for the top rated items are returned. Execute this code and examine the results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa621c-9e87-467c-a4a7-15ca28501497",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "4faa621c-9e87-467c-a4a7-15ca28501497",
    "outputId": "81e4f8f0-e7b0-4c6a-984a-50cf15b9a365"
   },
   "outputs": [],
   "source": [
    " def get_recommendations_for_user(predicted_ratings, ratings, user_id, n_recommendation):\n",
    "    \"\"\"\n",
    "    Function returns item ids of recommendations given a user_id\n",
    "    \"\"\"\n",
    "    ## Get the predicted ratings\n",
    "    user_predicted_ratings = predicted_ratings[user_id, :]\n",
    "\n",
    "    # Find unrated items for the user (assuming R_sparse has 0 for unrated)\n",
    "    unrated_item_indices = np.where(ratings[user_id, :].toarray()[0] == 0)[0]\n",
    "\n",
    "    # Get predicted ratings for unrated items\n",
    "    predictions_for_unrated = user_predicted_ratings[0, unrated_item_indices]\n",
    "    print(predictions_for_unrated.shape)\n",
    "    recommendations = pd.DataFrame({'item_index':unrated_item_indices, 'rating': predictions_for_unrated.toarray()[0]})\n",
    "\n",
    "    # Sort and return top recommendations\n",
    "    return recommendations.sort_values(by='rating', ascending=False).iloc[:n_recommendation,:]\n",
    "\n",
    "user = 0\n",
    "n_recommendation = 10\n",
    "recommendation_indicies = get_recommendations_for_user(SVD_rating_prediction,\n",
    "                                                       centered_spare_ratings_train,\n",
    "                                                       user, n_recommendation)\n",
    "\n",
    "recommendation_indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a197c2c-6cff-4590-9f7e-a6c5ad21f6e3",
   "metadata": {
    "id": "2a197c2c-6cff-4590-9f7e-a6c5ad21f6e3"
   },
   "source": [
    "To make this recommendation list human-understandable, it is easy to find the movie titles, using the `item_id`. Execute the code in the cell below and examine the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7950e519-a5f1-4473-a854-428e92684307",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "7950e519-a5f1-4473-a854-428e92684307",
    "outputId": "27f21190-2dbf-4fc9-920b-0c22fabb531c"
   },
   "outputs": [],
   "source": [
    "movies.loc[recommendation_indicies['item_index'],'movie title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5250c-8975-4d60-a8bf-fb3ee96f13a0",
   "metadata": {
    "id": "05f5250c-8975-4d60-a8bf-fb3ee96f13a0"
   },
   "source": [
    "> **Exercise 6-09:** Notice that the recommendations computed with the kNN collaborative filtering algorithm and the matrix factorization algorithm are completely different for the same user. Beyond possible small accuracy differences between the algorithms, what does this situation tell you about the difficulty of making good recommendations from rating data?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22d79a-52fd-4f38-9e27-2060c23dcd78",
   "metadata": {
    "id": "4b22d79a-52fd-4f38-9e27-2060c23dcd78"
   },
   "source": [
    "> **Answer:**        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81671da-306b-4918-8ac7-28607e600211",
   "metadata": {},
   "source": [
    "## Conceptual Questions   \n",
    "\n",
    "We will complete this assignment with a short set of conceptual questions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e591f2b-ac94-4f33-a06f-d5264e785a3d",
   "metadata": {
    "id": "d523104c-91e3-4a5b-9ae6-59c0ee28fa25"
   },
   "source": [
    "> **Exercise 6-10:** Recommender systems must overcome a number of biases. Answer the following questions concerning these biases in one or a few sentence.\n",
    "> 1. Explain how negative sampling bias arises in ranking algorithms.\n",
    "> 2. Explain how position bias arises in ranking algorithms.\n",
    "> 3. How are negative sampling bias and position bias related?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a6322-d38f-4895-a13f-7e75a273ca8c",
   "metadata": {
    "id": "11b8343b-36ad-4b52-b82b-37d147204f25"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.      \n",
    "> 2.        \n",
    "> 3.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e73d4-4623-480e-91be-beaa88e20198",
   "metadata": {
    "id": "30ea52ac-8361-4b68-8774-0c74698dbd4f"
   },
   "source": [
    "> **Exercise 6-11:** Embedding is essential for similarity search required for search and recommender algorithms. Answer the following questions in one or a few sentences.\n",
    "> 1. Provide an example of high-dimensional categorical variables used in recommender algorithms.   \n",
    "> 2. Why is embedding of categorical features essential for recommender systems.\n",
    "> 3. Explain the consequences of using a hash algorithm that does not have each of the four properties are essential for hashing a high dimensional categorical variable?\n",
    "> 4. If you need to use multiple high dimensional categorical variables in a ranking algorithm how can you formulate a vector for similarity search?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce290b15-cd10-476a-a184-8d97ea000911",
   "metadata": {
    "id": "9400aaee-0300-4632-bf8e-e931dc6ecb70"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.       \n",
    "> 2.      \n",
    "> 3.                \n",
    "> 4.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52091771-b7b5-429b-b656-e939118de530",
   "metadata": {
    "id": "f6e4d026-5eb6-4d77-983b-1d30cfe15236"
   },
   "source": [
    "> **Exercise 6-12:** In the precious assignment you worked with models that incorporate concepts from content-based recommenders. Consider the key concepts that define content based recommenders and answer the following questions in one or a few sentance.:\n",
    "> 1. What type of search is used to generate the recommendation list?\n",
    "> 2. How are recommendations ranked in a content-based recommender?\n",
    "> 3. How can a item-item content-based recommender overcome the cold start problem?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf5dc9-e52d-4b2a-90fa-f7e9625111c8",
   "metadata": {
    "id": "dfc684a6-b502-4ccb-a49a-ccba92d3dab0"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.       \n",
    "> 2.    \n",
    "> 3.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a469bc2c-12ee-4250-9ef3-7834c2771a74",
   "metadata": {
    "id": "b883802e-0ee0-4506-b4d6-d1c50ec3aa29"
   },
   "source": [
    "> **Exercise 6-13:** In one or a few sentences answer the following questions about collaborative filtering algorithms.\n",
    "> 1. How do mixed negative sampling (MNS) help with negative sampling bias for collaborative filtering algorithms?\n",
    "> 2. In a two tower model what is the role of the observation tower?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00375be-41ce-45c6-aa1f-8875a8154beb",
   "metadata": {
    "id": "091b1856-014b-4396-af5b-e5e8bbc2a118"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.     \n",
    "> 2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6243fd30-e844-4d71-b26f-96f4bef06536",
   "metadata": {
    "id": "6243fd30-e844-4d71-b26f-96f4bef06536"
   },
   "source": [
    "> **Exercise 6-14**: Previously, you worked with similarity search on movie attributes. When ranked, these results are a content based recommendations. Here you have worked with collaborative filtering algorithms. It is often a useful step to create a hybrid recommendation algorithm combining aspects of collaborative filtering and content based recommendation algorithms since both are based on similarity searches.\n",
    "> 1. How could you extend the utility matrix you have been working with to create a representation suitable for use with a hybrid algorithm in a fairly simple manner?\n",
    "> 2. Can the same extension to the utility matrix you have discussed also be used to add user context information?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4JBqJpYNlNH3",
   "metadata": {
    "id": "4JBqJpYNlNH3"
   },
   "source": [
    "> **Answers:**\n",
    "> 1.        \n",
    "> 2.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50433d-a456-46ed-8d4e-886905c2f0f2",
   "metadata": {
    "id": "db50433d-a456-46ed-8d4e-886905c2f0f2"
   },
   "source": [
    "#### Copyright 2025 Stephen F. Elston. All rights reserved.  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
